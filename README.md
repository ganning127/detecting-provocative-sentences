# Identifying Provocative Sentences from Fearful WhatsApp Messages using Machine Learning
*Authors: Ganning Xu, Vaibhav Garg*

## Abstract
According to the University of Maine, over 4 billion people use social media platforms daily. Additionally, in India, people often identify with a particular group based on religion and their native language. However, these same attributes are often misused by politicians to provoke these groups against each other, such as when 3,000 Sikhs were murdered over a span of three days in 1984 in the anti-Sikh riots. By leveraging an existing dataset of WhatsApp posts collected from Indian political groups, we identify three types of provocative and non-provocative sentences. We manually annotate 7,000 sentences into one of the four categories based on provocation. Next, we perform data analysis and find that provocative sentences are more pessimistic, more toxic, and contain more negative topics than non-provocative sentences. We train non-transformer-based machine learning models (Multinomial Naive Bayes, Logistic Regression, Random Forest, LinearSVC) combined with various embedding systems (word2vec, TFIDF, Universal Sentence Encoder) on our dataset. We also trained state-of-the-art transformer-based models (BERT, RoBERTa, DistilBERT, XLNet, DeBERTa). We determined that transformer-based models far outperform non-transformer-based ones. Using DeBERTa, we achieved 99\% recall, 99\% precision, and 99\% F1-Score using 10-fold stratified cross-validation. Out of the non-transformer-based approaches, LinearSVC performs the best, with 89\% recall, 89\% precision, and 89\% F1-Score. With any of these models, social media companies like WhatsApp can implement them into their apps, creating an efficient method for provocative sentence detection. We have made our code and dataset publicly available for all researchers.


## Contributions of Research
1. A **[7,000-sentence annotated dataset](https://github.com/ganning127/identifying-provocative-sentences/blob/main/data/labeled_data.csv)** with each sentence annotated as one of four categories: non-provocative, culturally provocative, oppressive provocative, and action provocative
2. **[Data analysis](https://github.com/ganning127/identifying-provocative-sentences/blob/main/annotation_analysis.ipynb)** to determine the textual differences in provocative and non-provocative sentences.
3. A thorough analysis of the effectiveness of non-transformer (**[TF-IDF](https://github.com/ganning127/identifying-provocative-sentences/blob/main/non_transformer_tfidf.ipynb)**, **[Word2vec](https://github.com/ganning127/identifying-provocative-sentences/blob/main/non_transformer_word2vec.ipynb)**, **[Universal Sentence Encoder](https://github.com/ganning127/identifying-provocative-sentences/blob/main/non_transformer_universal_sentence_encoder.ipynb)**) and **[transformer-based](https://github.com/ganning127/identifying-provocative-sentences/blob/main/transformer_models.ipynb)** models on provocative sentence classification
